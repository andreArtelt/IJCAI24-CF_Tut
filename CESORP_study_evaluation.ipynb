{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Load the Data](#1.-Load-the-Data)\n",
    "2. [Some First Eyeballing](#2.-Some-First-Eyeballing)\n",
    "3. [Data Cleaning](#3.-Data-Cleaning)\n",
    "4. [Eyeballing After Cleaning](#4.-Eyeballing-After-Cleaning)\n",
    "5. [Descriptive Statistics](#5.-Descriptive-Statistics)\n",
    "    - [Average Time Needed](#Average-Time-Needed)\n",
    "    - [Visualize Demographics](#Visualize-Demographics-(before-and-after-cleaning))\n",
    "    - [Test For Age Differences](#Test-For-Age-Differences)\n",
    "    - [Test For Gender Differences](#Test-For-Gender-Differences)\n",
    "\n",
    "\n",
    "6. [Statistical Analysis (finally!)](#6.-Statistical-Analysis-(finally!)) \n",
    "    - [First: Accuracy](#First:-Accuracy)\n",
    "    - [Second: User Judgements from Survey](#Second:-User-Judgements-from-Survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCxVWdgs2dDG"
   },
   "source": [
    "# 1. Load the Data\n",
    "First, we'll load all the CSV files into pandas DataFrames. We will also do some slight re-coding of the data for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scipy scikit_posthocs statsmodels rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPlTh7jO2RAW"
   },
   "outputs": [],
   "source": [
    "# Step A: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Step B: Load the data\n",
    "demographics = pd.read_csv('https://raw.githubusercontent.com/andreArtelt/IJCAI24-CF_Tut/main/CESORP_data_final/demographics.csv')\n",
    "reaction_times_instructions = pd.read_csv('https://raw.githubusercontent.com/andreArtelt/IJCAI24-CF_Tut/main/CESORP_data_final/reactionTimesInstructions.csv')\n",
    "reaction_times_task = pd.read_csv('https://raw.githubusercontent.com/andreArtelt/IJCAI24-CF_Tut/main/CESORP_data_final/reactionTimesTask.csv')\n",
    "survey_data = pd.read_csv('https://raw.githubusercontent.com/andreArtelt/IJCAI24-CF_Tut/main/CESORP_data_final/surveyData.csv')\n",
    "task_performance = pd.read_csv('https://raw.githubusercontent.com/andreArtelt/IJCAI24-CF_Tut/main/CESORP_data_final/taskPerformance.csv')\n",
    "\n",
    "# Step C: task performance, re-code Y to 1 and N to 0, add accuracy\n",
    "task_performance['userPred'] = task_performance['userPred'].map({'Y': 1, 'N': 0})\n",
    "task_performance['truePred'] = task_performance['truePred'].map({'Y': 1, 'N': 0})\n",
    "task_performance['accuracy'] = (task_performance['userPred'] == task_performance['truePred']).astype(int)\n",
    "\n",
    "# Step D: demographics, clean up superflous whitespace in categorical columns\n",
    "demographics['gender'] = demographics['gender'].str.replace(r'\\s{2,}', ' ', regex=True).str.strip()\n",
    "demographics['age'] = demographics['age'].astype(str).str.replace(r'\\s{2,}', ' ', regex=True).str.strip()\n",
    "\n",
    "# Step E: Define a consistent color palette for consistent plotting\n",
    "color_palette = {\n",
    "    's-cfe': '#1f77b4',  # blue\n",
    "    'o-cfe': '#ff7f0e',  # orange\n",
    "    's-con': '#2ca02c',  # green\n",
    "    'o-con': '#d62728'   # red\n",
    "}\n",
    "\n",
    "# Convert the palette to a list of colors\n",
    "colors = [color_palette[key] for key in color_palette]\n",
    "\n",
    "# Step F: Define a consistent color palette for consistent plotting\n",
    "color_palette_interaction = {\n",
    "    'cfe': '#1f77b4',  # blue\n",
    "    'con': '#ff7f0e',  # orange\n",
    "}\n",
    "\n",
    "# Convert the palette to a list of colors\n",
    "colors = [color_palette[key] for key in color_palette]\n",
    "\n",
    "#reaction_times_instructions['totalTimeMins']=reaction_times_instructions['totalTime']/1000/60\n",
    "#print(reaction_times_instructions.sort_values(by=['totalTime'], ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Some First Eyeballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_ylim(data, group_column, value_column):\n",
    "    grouped = data.groupby(group_column)[value_column].agg(['mean', 'sem']).reset_index()\n",
    "    max_ylim = (grouped['mean'] + 1.5*grouped['sem']).max()\n",
    "    return 0, max_ylim\n",
    "\n",
    "def get_material_ylim(data, group_column, material_column, value_column):\n",
    "    grouped = data.groupby([group_column, material_column])[value_column].agg(['mean', 'sem']).reset_index()\n",
    "    max_ylim = (grouped['mean'] + 1.2*grouped['sem']).max()\n",
    "    return 0, max_ylim\n",
    "\n",
    "# Initial plotting for eye-balling the data\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Determine the maximum count for setting the y-limits for demographics\n",
    "age_counts = demographics.groupby(['age', 'cond']).size().max()\n",
    "gender_counts = demographics.groupby(['gender', 'cond']).size().max()\n",
    "max_count_demo = max(age_counts, gender_counts)\n",
    "\n",
    "# Define all possible categories for age and gender\n",
    "all_ages = ['18-24y', '25-34y', '35-44y', '45-54y', '55-65y', '65y and older', 'prefer not to say']  # Assuming ages range from 20 to 30\n",
    "all_genders = ['male', 'female', 'non-binary', 'transgender', 'other', 'prefer not to say']  # Assuming genders include Male, Female, Other\n",
    "\n",
    "# Convert age and gender columns to category type with all possible categories\n",
    "demographics['age'] = pd.Categorical(demographics['age'], categories=all_ages)\n",
    "demographics['gender'] = pd.Categorical(demographics['gender'], categories=all_genders)\n",
    "\n",
    "# Plot Age distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=demographics, x='age', hue='cond',palette=color_palette)\n",
    "plt.title('Age Distribution by Condition')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo + 1)\n",
    "\n",
    "# Plot Gender distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=demographics, x='gender', hue='cond',palette=color_palette)\n",
    "plt.title('Gender Distribution by Condition')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo + 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look at Accuracy\n",
    "# add a column 'orderedTrial' reflecting the temporal order of trials per participant\n",
    "task_performance = task_performance.copy()\n",
    "task_performance['orderedTrial'] = task_performance.groupby('userID').cumcount() + 1\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy per user\n",
    "# Compute mean accuracy and merge with demographics for plotting\n",
    "mean_accuracy_per_user = task_performance.groupby('userID')['accuracy'].mean().reset_index()\n",
    "mean_accuracy_per_user = mean_accuracy_per_user.merge(demographics, on='userID')\n",
    "sns.barplot(data=mean_accuracy_per_user, x='cond', y='accuracy', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Accuracy per User by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.ylim(get_ylim(mean_accuracy_per_user, 'cond', 'accuracy'))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy over users of each group over time\n",
    "# (= trials chronologically ordered, to eyeball effects related to the duration of the study, fatigue, etc.)\n",
    "mean_performance_per_trial = task_performance.groupby(['cond', 'orderedTrial']).agg({'accuracy': 'mean'}).reset_index()\n",
    "sns.lineplot(data=mean_performance_per_trial, x='orderedTrial', y='accuracy', hue='cond', marker='o', palette=color_palette)\n",
    "plt.title('Mean Accuracy Over Users of Each Group on Each Trial - Chronologically Ordered')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy over users of each group on each material as bar plots\n",
    "sns.barplot(data=task_performance, x='trialNo', y='accuracy', hue='cond', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Accuracy Over Users of Each Group on Each Material')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.ylim(get_material_ylim(task_performance, 'cond', 'trialNo', 'accuracy'))\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look at Reaction Times\n",
    "# add a column 'orderedTrial' reflecting the temporal order of trials per participant\n",
    "reaction_times_task = reaction_times_task.copy()\n",
    "reaction_times_task['orderedTrial'] = reaction_times_task.groupby('userID').cumcount() + 1\n",
    "\n",
    "max_ylim_times_mean=max(get_ylim(reaction_times_task, 'cond', 'responseTime'),get_ylim(reaction_times_task, 'cond', 'explanationTime'))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Reaction Times\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=reaction_times_task, x='cond', y='responseTime', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Response Time by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Response Time (ms)')\n",
    "plt.ylim(max_ylim_times_mean)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=reaction_times_task, x='cond', y='explanationTime', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Explanation Time by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Explanation Time (ms)')\n",
    "plt.ylim(max_ylim_times_mean)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot reaction times per user, divided into groups for more visibility\n",
    "plt.figure(figsize=(8, 12))\n",
    "groups = reaction_times_task['cond'].unique()\n",
    "response_time_max = reaction_times_task['responseTime'].max()\n",
    "explanation_time_max = reaction_times_task['explanationTime'].max()\n",
    "\n",
    "# Trials Chronologically Ordered, to Eyeball Effects Related to the Duration of the Study, Fatigue, etc.)\n",
    "for i, group in enumerate(groups, 1):\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    subset = reaction_times_task[reaction_times_task['cond'] == group]\n",
    "    for user in subset['userID'].unique():\n",
    "        user_data = subset[subset['userID'] == user]\n",
    "        plt.plot(user_data['orderedTrial'], user_data['responseTime'], marker='o', linestyle='-', color='blue', alpha=0.6)\n",
    "        plt.plot(user_data['orderedTrial'], user_data['explanationTime'], marker='x', linestyle='--', color='red', alpha=0.6)\n",
    "    plt.title(f'Reaction Times for Group over Time: {group}')\n",
    "    plt.xlabel('Trial Number (Chronological)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.ylim(0, max(response_time_max, explanation_time_max))\n",
    "    plt.legend(['Response Time', 'Explanation Time'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "max_ylim_times=max(get_material_ylim(reaction_times_task, 'cond', 'trialNo', 'explanationTime'),get_material_ylim(reaction_times_task, 'cond', 'trialNo', 'responseTime'))\n",
    "\n",
    "# Plot Response Time per Material divided by group (to spot Effects Related to Materials)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='trialNo', y='responseTime', hue='cond', data=reaction_times_task, palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Response Time per Material Number')\n",
    "plt.xlabel('Material Number')\n",
    "plt.ylabel('Response Time (ms)')\n",
    "plt.ylim(max_ylim_times)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Explanation Time per Material divided by group (to spot Effects Related to Materials)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='trialNo', y='explanationTime', hue='cond', data=reaction_times_task, palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Explanation Time per Material Number')\n",
    "plt.xlabel('Material Number')\n",
    "plt.ylabel('Explanation Time (ms)')\n",
    "plt.ylim(max_ylim_times)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUcuSYgP_6q3"
   },
   "source": [
    "# 3. Data Cleaning\n",
    "Next, we'll clean the data based on pre-defined criteria.\n",
    "\n",
    "#### Outliers relative to population mean\n",
    "Specifically, we want to remove:\n",
    "\n",
    "- all speedsters  >3 SDs below the mean​\n",
    "- all materials  >3 SDs from mean​\n",
    "- materials >3 SDs from person‘s own mean, to catch attention failures​\n",
    "- straight-liners, repeatedly giving same answer, even with negative feedback​\n",
    "- non-varying responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ubYCNb3722uV",
    "outputId": "e11a6836-b671-4bc8-c058-d0a78ea3059d"
   },
   "outputs": [],
   "source": [
    "# Define a function for removing outliers\n",
    "def identify_outliers(df, column):\n",
    "    # define the factor with which std will be multiplied to find admissible range\n",
    "    removal_factor_population=3\n",
    "    print(\"\\nMeasure: \" + column)\n",
    "    print(\"Population mean: \" + str(round(df[column].mean(),3)))\n",
    "    print(\"Population std: \" + str(round(df[column].std(),3)))\n",
    "    print(\"Admissible range: \" + str(round(df[column].mean()-removal_factor_population * df[column].std())) + \" -- \" + str(round(df[column].mean()+removal_factor_population * df[column].std())))\n",
    "    #outliers = df[np.abs(df[column] - df[column].mean()) > (3 * df[column].std())]\n",
    "    outliers = df[df[column] < (df[column].mean() - removal_factor_population * df[column].std())]\n",
    "    return outliers['userID'].unique()\n",
    "\n",
    "# Identify speedsters: >3 SDs from population mean\n",
    "outliers_reaction_times_instructions = set()\n",
    "for column in ['timeConsentPage', 'timeInstruct1', 'timeInstruct2', 'timeAttentionCheck']:\n",
    "    outliers_reaction_times_instructions.update(identify_outliers(reaction_times_instructions, column))\n",
    "\n",
    "outliers_reaction_times_task = set()\n",
    "for column in ['responseTime', 'explanationTime']:\n",
    "    outliers_reaction_times_task.update(identify_outliers(reaction_times_task, column))\n",
    "    \n",
    "print(\"\\nUserIDs of speedsters relative to all participants (instructions) :\" + str(outliers_reaction_times_instructions))\n",
    "print(\"UserIDs of speedsters relative to all participants (task) :\" + str(outliers_reaction_times_task))\n",
    "\n",
    "# Identify straightliners (repeatedly giving the same answer) from survey data\n",
    "straightliners_survey = survey_data.groupby('userID').filter(lambda x: (x['responseNo'].var() == 0))\n",
    "print(\"\\nUserIDs of straightliners during survey: \" + str(straightliners_survey['userID'].unique()))\n",
    "\n",
    "# Identify straightliners (repeatedly giving the same answer) from task data\n",
    "straightliners_task = task_performance.groupby('userID').filter(lambda x: (x['userPred'].var() == 0))\n",
    "print(\"UserIDs of straightliners during task: \" + str(straightliners_task['userID'].unique()))\n",
    "\n",
    "# Identify attention failure in instruction (wrong answer to first question)\n",
    "attention_failures_instruction = reaction_times_instructions[(reaction_times_instructions['responseAttentionCheck-pass2'] == 0)]['userID'].unique()\n",
    "print(\"UserIDs of attention failures during instructions: \" + str(attention_failures_instruction))\n",
    "\n",
    "# Identify attention failure in survey (wrong answer to question X)\n",
    "attention_failures_survey = survey_data[(survey_data['itemNo'] == 5) & (survey_data['checked'] != 1) & (survey_data['responseNo'] == 2)]['userID'].unique()\n",
    "print(\"UserIDs of attention failures during survey: \" + str(attention_failures_survey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all population outliers\n",
    "all_outliers = outliers_reaction_times_instructions.union(outliers_reaction_times_task).union(straightliners_survey['userID'].unique()).union(straightliners_task['userID'].unique()).union(attention_failures_survey).union(attention_failures_instruction)\n",
    "\n",
    "print(\"UserIDs of all outliers: \" + str(all_outliers))\n",
    "print(\"N: \" + str(len(all_outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove population outliers from all datasets\n",
    "reaction_times_instructions_cleaned = reaction_times_instructions[~reaction_times_instructions['userID'].isin(all_outliers)]\n",
    "reaction_times_task_cleaned = reaction_times_task[~reaction_times_task['userID'].isin(all_outliers)]\n",
    "task_performance_cleaned = task_performance[~task_performance['userID'].isin(all_outliers)]\n",
    "survey_data_cleaned = survey_data[~survey_data['userID'].isin(all_outliers)]\n",
    "demographics_cleaned = demographics[~demographics['userID'].isin(all_outliers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wonky Materials?\n",
    "\n",
    "Are there any that are systematically answered unusually quickly?\n",
    "Here, we want to figure out if there are:\n",
    "\n",
    "- any materials  >3 SDs from polulation mean\n",
    "- any materials >3 SDs from person‘s own mean (to catch attention failures of individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify wonky materials: RTs >3 SDs from person‘s own mean\n",
    "# Define the factor with which std will be multiplied to find the admissible range\n",
    "removal_factor_user_wise = 3\n",
    "wonky_trial_user_percentage = 30\n",
    "\n",
    "# Identify trials (=materials) where users performed more quickly than > 3SD from their own RT mean\n",
    "response_time_outliers = reaction_times_task_cleaned[\n",
    "    reaction_times_task_cleaned['responseTime'] < (\n",
    "        reaction_times_task_cleaned.groupby('userID')['responseTime'].transform('mean') - \n",
    "        removal_factor_user_wise * reaction_times_task_cleaned.groupby('userID')['responseTime'].transform('std')\n",
    "    )\n",
    "]\n",
    "\n",
    "explanation_time_outliers = reaction_times_task_cleaned[\n",
    "    reaction_times_task_cleaned['explanationTime'] < (\n",
    "        reaction_times_task_cleaned.groupby('userID')['explanationTime'].transform('mean') - \n",
    "        removal_factor_user_wise * reaction_times_task_cleaned.groupby('userID')['explanationTime'].transform('std')\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display the outliers for responseTime and explanationTime\n",
    "print(\"\\nUserIDs + trialNo of speedsters relative to own performance (response time):\")\n",
    "print(response_time_outliers[['userID', 'trialNo', 'responseTime']])\n",
    "\n",
    "print(\"\\nUserIDs + trialNo of speedsters relative to own performance (explanation time):\")\n",
    "print(explanation_time_outliers[['userID', 'trialNo', 'explanationTime']])\n",
    "\n",
    "# Combine the outliers into a single DataFrame for analysis\n",
    "outliers_combined = pd.concat([\n",
    "    response_time_outliers[['userID', 'trialNo']],\n",
    "    explanation_time_outliers[['userID', 'trialNo']]\n",
    "]).drop_duplicates()\n",
    "\n",
    "# Calculate the percentage of outliers for each trialNo(=material)\n",
    "outlier_percentage = outliers_combined['trialNo'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Identify trialNo(=material) where at least 30% of participants were quicker than > 3 SDs of their own performance\n",
    "wonky_trials = outlier_percentage[outlier_percentage >= wonky_trial_user_percentage].index.tolist()\n",
    "\n",
    "print(\"\\nList of trialNo where at least 30% of participants were quicker than > 3 SDs of their own performance:\")\n",
    "print(wonky_trials)\n",
    "\n",
    "# Show specifics of wonky_trials (=material)\n",
    "fields = [\"trialNo\", \"truePred\", \"valQuality\", \"valProductivity\", \"valPunctuality\", \"valFeedback\"]\n",
    "wonky_trials_df = task_performance_cleaned[task_performance_cleaned['trialNo'].isin(wonky_trials)].groupby('trialNo').head(1)[fields]\n",
    "print(\"\\nSpecifics of wonky materials:\")\n",
    "print(wonky_trials_df)\n",
    "\n",
    "\n",
    "# Filter out wonky trials from the dataframes - CAREFUL! LIKELY BREAKS BALANCE OF TRIALS!\n",
    "#def remove_wonky_trials(df, wonky_trials):\n",
    "#    return df[~df['trialNo'].isin(wonky_trials)]\n",
    "# Remove wonky trials from all relevant dataframes\n",
    "#reaction_times_task_cleaned = remove_wonky_trials(reaction_times_task_cleaned, wonky_trials)\n",
    "#task_performance_cleaned = remove_wonky_trials(task_performance_cleaned, wonky_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Eyeballing After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate y-axis limit\n",
    "def get_ylim(data, group_column, value_column):\n",
    "    grouped = data.groupby(group_column)[value_column].agg(['mean', 'sem']).reset_index()\n",
    "    max_ylim = (grouped['mean'] + 1.5*grouped['sem']).max()\n",
    "    return 0, max_ylim\n",
    "\n",
    "def get_material_ylim(data, group_column, material_column, value_column):\n",
    "    grouped = data.groupby([group_column, material_column])[value_column].agg(['mean', 'sem']).reset_index()\n",
    "    max_ylim = (grouped['mean'] + 1.2*grouped['sem']).max()\n",
    "    return 0, max_ylim\n",
    "\n",
    "# Initial plotting for eye-balling the data\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Determine the maximum count for setting the y-limits for demographics\n",
    "age_counts = demographics_cleaned.groupby(['age', 'cond']).size().max()\n",
    "gender_counts = demographics_cleaned.groupby(['gender', 'cond']).size().max()\n",
    "max_count_demo = max(age_counts, gender_counts)\n",
    "\n",
    "# Plot Age distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=demographics_cleaned, x='age', hue='cond', palette=color_palette)\n",
    "plt.title('Age Distribution by Condition')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo + 1)\n",
    "\n",
    "# Plot Gender distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=demographics_cleaned, x='gender', hue='cond', palette=color_palette)\n",
    "plt.title('Gender Distribution by Condition')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo + 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look at Accuracy\n",
    "# add a column 'orderedTrial' reflecting the temporal order of trials per participant\n",
    "task_performance_cleaned = task_performance_cleaned.copy()\n",
    "task_performance_cleaned['orderedTrial'] = task_performance_cleaned.groupby('userID').cumcount() + 1\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy per user\n",
    "# Compute mean accuracy and merge with demographics for plotting\n",
    "mean_accuracy_per_user_cleaned = task_performance_cleaned.groupby('userID')['accuracy'].mean().reset_index()\n",
    "mean_accuracy_per_user_cleaned = mean_accuracy_per_user_cleaned.merge(demographics_cleaned, on='userID')\n",
    "sns.barplot(data=mean_accuracy_per_user_cleaned, x='cond', y='accuracy', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Accuracy per User by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.ylim(get_ylim(mean_accuracy_per_user_cleaned, 'cond', 'accuracy'))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy over users of each group over time\n",
    "# (= trials chronologically ordered, to eyeball effects related to the duration of the study, fatigue, etc.)\n",
    "mean_performance_per_trial_cleaned = task_performance_cleaned.groupby(['cond', 'orderedTrial']).agg({'accuracy': 'mean'}).reset_index()\n",
    "sns.lineplot(data=mean_performance_per_trial_cleaned, x='orderedTrial', y='accuracy', hue='cond', marker='o', palette=color_palette)\n",
    "plt.title('Mean Accuracy Over Users of Each Group on Each Trial - Chronologically Ordered')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy over users of each group on each material as bar plots\n",
    "sns.barplot(data=task_performance_cleaned, x='trialNo', y='accuracy', hue='cond', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Accuracy Over Users of Each Group on Each Material')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.ylim(get_material_ylim(task_performance_cleaned, 'cond', 'trialNo', 'accuracy'))\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look at Reaction Times\n",
    "# add a column 'orderedTrial' reflecting the temporal order of trials per participant\n",
    "reaction_times_task_cleaned = reaction_times_task_cleaned.copy()\n",
    "reaction_times_task_cleaned['orderedTrial'] = reaction_times_task_cleaned.groupby('userID').cumcount() + 1\n",
    "\n",
    "max_ylim_times_mean_cleaned = max(get_ylim(reaction_times_task_cleaned, 'cond', 'responseTime'), get_ylim(reaction_times_task_cleaned, 'cond', 'explanationTime'))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Reaction Times\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=reaction_times_task_cleaned, x='cond', y='responseTime', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Response Time by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Response Time (ms)')\n",
    "plt.ylim(max_ylim_times_mean_cleaned)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=reaction_times_task_cleaned, x='cond', y='explanationTime', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Explanation Time by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Explanation Time (ms)')\n",
    "plt.ylim(max_ylim_times_mean_cleaned)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot reaction times per user, divided into groups for more visibility\n",
    "plt.figure(figsize=(8, 12))\n",
    "groups_cleaned = reaction_times_task_cleaned['cond'].unique()\n",
    "\n",
    "# Trials Chronologically Ordered, to Eyeball Effects Related to the Duration of the Study, Fatigue, etc.)\n",
    "for i, group in enumerate(groups_cleaned, 1):\n",
    "    plt.subplot(len(groups_cleaned), 1, i)\n",
    "    subset_cleaned = reaction_times_task_cleaned[reaction_times_task_cleaned['cond'] == group]\n",
    "    for user in subset_cleaned['userID'].unique():\n",
    "        user_data_cleaned = subset_cleaned[subset_cleaned['userID'] == user]\n",
    "        plt.plot(user_data_cleaned['orderedTrial'], user_data_cleaned['responseTime'], marker='o', linestyle='-', color='blue', alpha=0.6)\n",
    "        plt.plot(user_data_cleaned['orderedTrial'], user_data_cleaned['explanationTime'], marker='x', linestyle='--', color='red', alpha=0.6)\n",
    "    plt.title(f'Reaction Times for Group over Time: {group}')\n",
    "    plt.xlabel('Trial Number (Chronological)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.ylim(0, max(response_time_max, explanation_time_max))\n",
    "    plt.legend(['Response Time', 'Explanation Time'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "max_ylim_times_cleaned = max(get_material_ylim(reaction_times_task_cleaned, 'cond', 'trialNo', 'explanationTime'), get_material_ylim(reaction_times_task_cleaned, 'cond', 'trialNo', 'responseTime'))\n",
    "\n",
    "# Plot Response Time per Material divided by group (to spot Effects Related to Materials)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='trialNo', y='responseTime', hue='cond', data=reaction_times_task_cleaned, palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Response Time per Material Number')\n",
    "plt.xlabel('Material Number')\n",
    "plt.ylabel('Response Time (ms)')\n",
    "plt.ylim(max_ylim_times_cleaned)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Explanation Time per Material divided by group (to spot Effects Related to Materials)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='trialNo', y='explanationTime', hue='cond', data=reaction_times_task_cleaned, palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Explanation Time per Material Number')\n",
    "plt.xlabel('Material Number')\n",
    "plt.ylabel('Explanation Time (ms)')\n",
    "plt.ylim(max_ylim_times_cleaned)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9IrlWuKLF-3"
   },
   "source": [
    "# 5. Descriptive Statistics\n",
    "We'll compute descriptive statistics for the demographics and check if groups are significantly different in terms of age and gender.\n",
    "\n",
    "## Average Time Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_time_cleaned=np.mean(reaction_times_instructions['totalTime'])\n",
    "print('Mean total time needed per participant:')\n",
    "print(str(round(mean_total_time_cleaned/1000/60,2)) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Demographics (before and after cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide a summary of the number of participants in each group\n",
    "participant_summary = demographics_cleaned.groupby('cond').size().reset_index(name='Number of Participants')\n",
    "\n",
    "print(\"\\nSummary of the number of participants in each group:\")\n",
    "print(participant_summary)\n",
    "\n",
    "# make some plots:\n",
    "# for uniform axes, get the frequency distribution of age / gender groups by condition\n",
    "age_distribution_by_cond = demographics.groupby(['cond', 'age']).size().reset_index(name='count')\n",
    "gender_distribution_by_cond = demographics.groupby(['cond', 'gender']).size().reset_index(name='count')\n",
    "\n",
    "# Find the maximal frequency for demographics groups by condition\n",
    "max_count_demo = max(max(age_distribution_by_cond.loc[age_distribution_by_cond.groupby('cond')['count'].idxmax()]['count']),max(gender_distribution_by_cond.loc[gender_distribution_by_cond.groupby('cond')['count'].idxmax()]['count']))\n",
    "\n",
    "# Plot: Before data cleaning\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot Age distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=demographics, x='age', hue='cond',palette=color_palette)\n",
    "plt.title('Age Distribution by Condition Before Cleaning')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo+1)\n",
    "\n",
    "# Plot Gender distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=demographics, x='gender', hue='cond',palette=color_palette)\n",
    "plt.title('Gender Distribution by Condition Before Cleaning')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo+1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot: After data cleaning\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot Age distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=demographics_cleaned, x='age', hue='cond',palette=color_palette)\n",
    "plt.title('Age Distribution by Condition After Cleaning')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo+1)\n",
    "\n",
    "# Plot Gender distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=demographics_cleaned, x='gender', hue='cond',palette=color_palette)\n",
    "plt.title('Gender Distribution by Condition After Cleaning')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, max_count_demo+1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test For Age Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFb4PaLN2bbv"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Perform Kruskal-Wallis H Test for age distributions across conditions\n",
    "groups = [demographics_cleaned[demographics_cleaned['cond'] == cond]['age'] for cond in demographics_cleaned['cond'].unique()]\n",
    "kruskal_test_result = kruskal(*groups)\n",
    "\n",
    "# Calculate Eta-Squared (η²)\n",
    "H = kruskal_test_result.statistic\n",
    "k = len(groups)\n",
    "n = len(demographics)\n",
    "eta_squared = (H - k + 1) / (n - 1)\n",
    "\n",
    "print(\"Kruskal-Wallis H Test for Age Distributions:\")\n",
    "print(f\"H-Statistic: {round(kruskal_test_result.statistic,3)}, p-value: {round(kruskal_test_result.pvalue,3)}\")\n",
    "print(f\"Eta-Squared (η²) Effect Size: {abs(round(eta_squared,3))}\")\n",
    "print(\"\\nNote about effect size Eta-squared η²: it indicates the proportion of the total variance of the dependent variable that can be explained by the independent variable. I.e., η² means that 20% of the variation observed in groups can be explained by age.\")\n",
    "\n",
    "# If the Kruskal-Wallis H test is significant, perform post hoc test: Dunn's Test\n",
    "if kruskal_test_result.pvalue < 0.05:\n",
    "    p_values_dunn_age = sp.posthoc_dunn(demographics_cleaned, val_col='age', group_col='cond', p_adjust='bonferroni')\n",
    "    print(\"Post Hoc Comparisons using Dunn's Test with Bonferroni Correction:\")\n",
    "    print(p_values_dunn_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test For Gender Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, chi2, norm, shapiro, levene, kruskal\n",
    "import itertools\n",
    "import statsmodels.stats.multitest as smm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Chi-Square Test of Independence for Gender Distributions\n",
    "contingency_table = pd.crosstab(demographics_cleaned['cond'], demographics_cleaned['gender'])\n",
    "chi2_test_result = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi^2 Test of Independence for Gender Distributions:\")\n",
    "print(f\"Chi^2 Statistic: {round(chi2_test_result[0],3)}, p-value: {round(chi2_test_result[1],3)}\")\n",
    "# if p > 0.05, H0 holds: we can assume that there is no dependence between age_group and condition\n",
    "\n",
    "# Function to perform pairwise z-tests for proportions, for potential post-hoc comparisons\n",
    "def pairwise_z_test(count1, nobs1, count2, nobs2):\n",
    "    prop1 = count1 / nobs1\n",
    "    prop2 = count2 / nobs2\n",
    "    prop_pool = (count1 + count2) / (nobs1 + nobs2)\n",
    "    se_pool = (prop_pool * (1 - prop_pool) * (1/nobs1 + 1/nobs2)) ** 0.5\n",
    "    z_stat = (prop1 - prop2) / se_pool\n",
    "    p_value = 2 * norm.cdf(-abs(z_stat))\n",
    "    return p_value\n",
    "\n",
    "# If the chi-square test is significant, perform post hoc tests\n",
    "if chi2_test_result[1] < 0.05:\n",
    "    # Generate all pairwise comparisons\n",
    "    comparisons = list(itertools.combinations(contingency_table.index, 2))\n",
    "    \n",
    "    p_values = []\n",
    "    for (group1, group2) in comparisons:\n",
    "        for gender in contingency_table.columns:\n",
    "            count1 = contingency_table.loc[group1, gender]\n",
    "            count2 = contingency_table.loc[group2, gender]\n",
    "            nobs1 = contingency_table.loc[group1].sum()\n",
    "            nobs2 = contingency_table.loc[group2].sum()\n",
    "            p_val = pairwise_z_test(count1, nobs1, count2, nobs2)\n",
    "            p_values.append(p_val)\n",
    "            print(f\"Comparison {group1} vs {group2} for gender {gender}: p-value = {p_val}\")\n",
    "    \n",
    "    # Adjust p-values for multiple comparisons using the Bonferroni correction\n",
    "    reject, pvals_corrected, _, _ = smm.multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "    \n",
    "    # Display adjusted p-values\n",
    "    for (group1, group2), p_val_corr in zip(comparisons, pvals_corrected):\n",
    "        print(f\"Adjusted p-value for {group1} vs {group2}: {p_val_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5nacQe32eqH"
   },
   "source": [
    "# 6. Statistical Analysis (finally!)\n",
    "We'll analyze the accuracy of prediction and the mean feature importances for each group.\n",
    "\n",
    "## First: Accuracy \n",
    "\n",
    "We will kick-off with the accuracy for group first. Initially, we planned to run a 2x2 ANOVA to compare mean performance of all groups. However, we have to keep in mind that ANOVA has two important assumptions: data needs to normally distributed and the variances need to be homogeneous. If these assumptions are not met, it is wiser to run the non-parametric alternative: Kruskal-Wallis H Test.\n",
    "\n",
    "Here is the plan: \n",
    "\n",
    "1. Plot the accuracies again to remember the distributions.\n",
    "2. Check Assumptions: Normality (Shapiro-Wilk test) and Homogeneity of Variances (Levene's test).\n",
    "3. Perform ANOVA if assumptions are met.\n",
    "4. If only normality is violated, perform ART ANOVA.\n",
    "5. If both normality and homoscedasticity are violated, perform Kruskal-Wallis H Test.\n",
    "\n",
    "### Step 1: Plot again to refresh our memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ncSS6y6_Goy"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Accuracy per user\n",
    "# Compute mean accuracy and merge with demographics for plotting\n",
    "mean_accuracy_per_user_cleaned = task_performance_cleaned.groupby('userID')['accuracy'].mean().reset_index()\n",
    "mean_accuracy_per_user_cleaned = mean_accuracy_per_user_cleaned.merge(demographics_cleaned, on='userID')\n",
    "sns.barplot(data=mean_accuracy_per_user_cleaned, x='cond', y='accuracy', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Accuracy per User by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.ylim(get_ylim(mean_accuracy_per_user_cleaned, 'cond', 'accuracy'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Normality using Shapiro-Wilk test (if p<0.05, assume data is NOT normal)\n",
    "normality_results = task_performance_cleaned.groupby('cond')['accuracy'].apply(shapiro)\n",
    "normality_passed = all(p.pvalue > 0.05 for p in normality_results)\n",
    "print(\"Shapiro-Wilk Test for Normality:\")\n",
    "for cond, result in normality_results.items():\n",
    "    print(f\"Condition {cond}: W-statistic = {round(result.statistic,3)}, p-value = {round(result.pvalue,3)}\")\n",
    "\n",
    "# Check Homogeneity of Variances using Levene's test (if p<0.05, assume data is NOT homoscedastic)\n",
    "levene_stat, levene_pvalue = levene(*[task_performance_cleaned[task_performance_cleaned['cond'] == cond]['accuracy'] for cond in task_performance_cleaned['cond'].unique()])\n",
    "homogeneity_passed = levene_pvalue > 0.05\n",
    "print(f\"\\nLevene's Test for Homogeneity of Variances: Statistic = {round(levene_stat,3)}, p-value = {round(levene_pvalue,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3-5: Perform ANOVA if assumptions are met, ART ANOVA if partially met, & Kruskal-Wallis H Test otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "\n",
    "# Generate \"referent\" and \"exp\" columns using assign to avoid SettingWithCopyWarning\n",
    "task_performance_cleaned = task_performance_cleaned.assign(\n",
    "    referent=task_performance_cleaned['cond'].str[0],\n",
    "    exp=task_performance_cleaned['cond'].str.split('-').str[1]\n",
    ")\n",
    "\n",
    "# Convert \"referent\" and \"exp\" columns to categorical type in Python\n",
    "task_performance_cleaned['referent'] = task_performance_cleaned['referent'].astype('category')\n",
    "task_performance_cleaned['exp'] = task_performance_cleaned['exp'].astype('category')\n",
    "\n",
    "print(\"\\nVisualize results\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.pointplot(data=task_performance_cleaned, x='referent', y='accuracy', hue='exp', palette=color_palette_interaction, dodge=True, markers=['o', 's'], capsize=0.1, errwidth=1, errorbar='se')\n",
    "plt.title('Interaction Plot: Mean Accuracy by Referent and Exp')\n",
    "plt.xlabel('Referent')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend(title='Exp')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if normality_passed and homogeneity_passed:\n",
    "    # Perform 2x2 ANOVA\n",
    "    model = ols('accuracy ~ C(referent) * C(exp)', data=task_performance_cleaned).fit()\n",
    "    anova_results = anova_lm(model)\n",
    "    print(\"ANOVA Results:\")\n",
    "    print(round(anova_results,3))\n",
    "    print(\"\\nANOVA results table explained:\")\n",
    "    print(\"1. row: results for main effect of factor 'referent'\")\n",
    "    print(\"2. row: results for main effect of factor 'exp'\")\n",
    "    print(\"3. row: results for the interaction of both factors. If this is singificant, the effect of one factor depends on the value of the other factor.\")\n",
    "    print(\"4. row: residual, the variation in the dependent variable (accuracy) that isn't explained by the independent variables (referent or factor)\")\n",
    "    # Perform post hoc test if interaction of ANOVA is significant\n",
    "    if anova_results[\"PR(>F)\"][2] < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(task_performance_cleaned, val_col='accuracy', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"\\nPost Hoc Comparisons (Interaction) using Dunn's Test:\")\n",
    "        print(round(posthoc_result,3))\n",
    "        \n",
    "else:\n",
    "    # Perform Kruskal-Wallis H Test for Accuracy\n",
    "    groups = [task_performance_cleaned[task_performance_cleaned['cond'] == cond]['accuracy'] for cond in task_performance_cleaned['cond'].unique()]\n",
    "    kruskal_test_result = kruskal(*groups)\n",
    "\n",
    "    # Calculate Eta-Squared (η²)\n",
    "    H = kruskal_test_result.statistic\n",
    "    k = len(groups)\n",
    "    n = len(task_performance_cleaned)\n",
    "    eta_squared = (H - k + 1) / (n - 1)\n",
    "\n",
    "    print(\"Kruskal-Wallis H Test for Accuracy:\")\n",
    "    print(f\"Statistic: {round(kruskal_test_result.statistic,3)}, p-value: {round(kruskal_test_result.pvalue,3)}\")\n",
    "    print(f\"Eta-Squared (η²) Effect Size: {round(eta_squared,3)}\")\n",
    "\n",
    "    # Perform Dunn's test for post hoc comparisons if Kruskal-Wallis test is significant\n",
    "    if kruskal_test_result.pvalue < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(task_performance_cleaned, val_col='accuracy', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"Post Hoc Comparisons using Dunn's Test with Bonferroni Correction:\")\n",
    "        print(round(posthoc_result,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7G0Bhvc_KLO"
   },
   "source": [
    "## Second: User Judgements from Survey\n",
    "\n",
    "Let's turn to the survey information. Two measures will be considered:\n",
    "\n",
    "A. Distance between ground truth importance and user evaluation\n",
    "\n",
    "B. Analysis of Survey Responses\n",
    "\n",
    "### A. Compute and Evaluate Distance Measure Between Ground Truth Importance And User Judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep only rows where itemNo == 1 and checked == 1\n",
    "survey_data_importance = survey_data_cleaned[(survey_data_cleaned['itemNo'] == 1) & (survey_data_cleaned['checked'] == 1)]\n",
    "\n",
    "# Drop duplicate rows based on userID, keeping only the first occurrence\n",
    "survey_data_importance = survey_data_importance.drop_duplicates(subset='userID')\n",
    "\n",
    "# Define a function to transform the values into \"high\", \"medium\", and \"low\" based on quantiles\n",
    "def transform_quantile_ranking(row):\n",
    "    scores = [\n",
    "        row['importanceQuality'],\n",
    "        row['importanceProductivity'],\n",
    "        row['importancePunctuality'],\n",
    "        row['importanceFeedback']\n",
    "    ]\n",
    "    high_threshold = pd.Series(scores).quantile(0.66)\n",
    "    low_threshold = pd.Series(scores).quantile(0.33)\n",
    "    return [\n",
    "        'high' if score > high_threshold else 'medium' if score > low_threshold else 'low' \n",
    "        for score in scores\n",
    "    ]\n",
    "\n",
    "# Apply the transformation to the relevant columns using assign\n",
    "rankings = survey_data_importance.apply(transform_quantile_ranking, axis=1)\n",
    "rankings_df = pd.DataFrame(rankings.tolist(), columns=['Quality', 'Productivity', 'Punctuality', 'Feedback'])\n",
    "\n",
    "survey_data_importance = pd.concat([survey_data_importance.reset_index(drop=True), rankings_df], axis=1)\n",
    "\n",
    "# Compute the distance measure based on the specified criteria\n",
    "def compute_distance_measure(row):\n",
    "    score = 0\n",
    "    if row['Quality'] == 'high':\n",
    "        score += 1\n",
    "    if row['Productivity'] == 'high':\n",
    "        score += 1\n",
    "    if row['Punctuality'] == 'medium':\n",
    "        score += 1\n",
    "    if row['Feedback'] == 'low':\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "# Apply the function to compute the distance measure\n",
    "survey_data_importance['distance_measure'] = survey_data_importance.apply(compute_distance_measure, axis=1)\n",
    "\n",
    "# Visualize mean per group for the distance_measure\n",
    "plt.figure(figsize=(8, 4))\n",
    "# Plot Mean Distance Measure per User\n",
    "sns.barplot(data=survey_data_importance, x='cond', y='distance_measure', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Distance to Ground Truth by Condition (max=4, min=0)')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Distance Measure')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# generate factor columns\n",
    "survey_data_importance['referent'] = survey_data_importance['cond'].str[0]  # 's' or 'o'\n",
    "survey_data_importance['exp'] = survey_data_importance['cond'].str.split('-').str[1]  # 'cfe' or 'con'\n",
    "# Convert \"referent\" and \"exp\" columns to categorical type in Python\n",
    "survey_data_importance['referent'] = survey_data_importance['referent'].astype('category')\n",
    "survey_data_importance['exp'] = survey_data_importance['exp'].astype('category')\n",
    "\n",
    "# Check assumption 1) Normality (if Shapiro-Wilk p<0.05, assume data is NOT normal)\n",
    "normality_results = survey_data_importance.groupby('cond')['distance_measure'].apply(shapiro)\n",
    "normality_passed = all(p.pvalue > 0.05 for p in normality_results)\n",
    "print(\"Shapiro-Wilk Test for Normality:\")\n",
    "for cond, result in normality_results.items():\n",
    "    print(f\"Condition {cond}: W-statistic = {round(result.statistic,3)}, p-value = {round(result.pvalue,3)}\")\n",
    "\n",
    "# Check Homogeneity of Variances using Levene's test (if p<0.05, assume data is NOT homoscedastic)\n",
    "levene_stat, levene_pvalue = levene(*[survey_data_importance[survey_data_importance['cond'] == cond]['distance_measure'] for cond in survey_data_importance['cond'].unique()])\n",
    "homogeneity_passed = levene_pvalue > 0.05\n",
    "print(f\"\\nLevene's Test for Homogeneity of Variances: Statistic = {round(levene_stat,3)}, p-value = {round(levene_pvalue,3)}\")\n",
    "\n",
    "print(\"\\nVisualize results\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.pointplot(data=survey_data_importance, x='referent', y='distance_measure', hue='exp', palette=color_palette_interaction, dodge=True, markers=['o', 's'], capsize=0.1, errwidth=1, errorbar='se')\n",
    "plt.title('Interaction Plot: Mean Distance Measure by Referent and Exp')\n",
    "plt.xlabel('Referent')\n",
    "plt.ylabel('Mean Distance from Ground Truth')\n",
    "plt.legend(title='Exp')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if normality_passed and homogeneity_passed:\n",
    "    # Perform 2x2 ANOVA\n",
    "    model = ols('distance_measure ~ C(referent) * C(exp)', data=survey_data_importance).fit()\n",
    "    anova_results = anova_lm(model)\n",
    "    print(\"ANOVA Results:\")\n",
    "    print(round(anova_results,3))\n",
    "    print(\"\\nANOVA results table explained:\")\n",
    "    print(\"1. row: results for main effect of factor 'referent'\")\n",
    "    print(\"2. row: results for main effect of factor 'exp'\")\n",
    "    print(\"3. row: results for the interaction of both factors. If this is singificant, the effect of one factor depends on the value of the other factor.\")\n",
    "    print(\"4. row: residual, the variation in the dependent variable (accuracy) that isn't explained by the independent variables (referent or factor)\")\n",
    "    \n",
    "    # Perform post hoc test if interaction of ANOVA is significant\n",
    "    if anova_results[\"PR(>F)\"][2] < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_importance, val_col='distance_measure', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"\\nPost Hoc Comparisons (Interaction) using Dunn's Test:\")\n",
    "        print(round(posthoc_result,3))\n",
    "        \n",
    "else:\n",
    "    # Perform Kruskal-Wallis H Test for Accuracy\n",
    "    groups = [survey_data_importance[survey_data_importance['cond'] == cond]['distance_measure'] for cond in survey_data_importance['cond'].unique()]\n",
    "    kruskal_test_result = kruskal(*groups)\n",
    "\n",
    "    # Calculate Eta-Squared (η²)\n",
    "    H = kruskal_test_result.statistic\n",
    "    k = len(groups)\n",
    "    n = len(task_performance_cleaned)\n",
    "    eta_squared = (H - k + 1) / (n - 1)\n",
    "\n",
    "    print(\"Kruskal-Wallis H Test for Distance Masure:\")\n",
    "    print(f\"Statistic: {round(kruskal_test_result.statistic,3)}, p-value: {round(kruskal_test_result.pvalue,3)}\")\n",
    "    print(f\"Eta-Squared (η²) Effect Size: {round(eta_squared,3)}\")\n",
    "\n",
    "    # Perform Dunn's test for post hoc comparisons if Kruskal-Wallis test is significant\n",
    "    if kruskal_test_result.pvalue < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_importancetask_performance_cleaned, val_col='distance_measure', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"Post Hoc Comparisons using Dunn's Test with Bonferroni Correction:\")\n",
    "        print(round(posthoc_result,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Analyze Survey Responses Regarding Satisfaction and Trust\n",
    "\n",
    "#### Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only rows where checked == 1\n",
    "survey_data_checked = survey_data_cleaned[survey_data_cleaned['checked'] == 1]\n",
    "\n",
    "# Define the items for Explanation Satisfaction Scale and Explanation Trust Scale\n",
    "satisfaction_items = [0, 1, 2, 3, 4, 6, 7, 8] #[1, 2, 3, 4, 5, 7, 8, 9] \n",
    "trust_items = [9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "# Define labels for the plots\n",
    "satisfaction_labels = {\n",
    "    0: 'Understanding', 1: 'Satisfaction (reverse scored)', 2: 'Sufficient detail', 3: 'Completeness',\n",
    "    4: 'How to use', 6: 'Usefulness (reverse scored)', 7: 'Accuracy', 8: 'Appropriate trust'\n",
    "}\n",
    "\n",
    "trust_labels = {\n",
    "    9: 'Confidence (reverse scored)', 10: 'Predictability', 11: 'Reliance', 12: 'Fair Evaluation (reverse scored)',\n",
    "    13: 'Efficiency', 14: 'Wariness (reverse scored)', 15: 'Improvement over human performance', 16: 'Liking of system'} ##, 17: 'Liking of system}\n",
    "\n",
    "# Plot average judgments per group for Explanation Satisfaction Scale\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, item in enumerate(satisfaction_items, 1):\n",
    "    plt.subplot(2, 4, i)\n",
    "    sns.barplot(data=survey_data_checked[survey_data_checked['itemNo'] == item], x='cond', y='responseNo', palette=color_palette, errorbar='se')\n",
    "    plt.title(satisfaction_labels[item])\n",
    "    plt.xlabel('Condition')\n",
    "    plt.ylabel('Mean Judgment')\n",
    "plt.suptitle('Mean Judgments for Explanation Satisfaction Scale')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recode reverse scored items\n",
    "survey_data_checked = survey_data_checked.assign(\n",
    "    response_recode=survey_data_checked['responseNo']\n",
    ")\n",
    "\n",
    "# To get a Correct Summed Score, reverse code relevant items\n",
    "survey_data_checked.loc[survey_data_checked['itemNo'].isin([1, 6, 9, 12, 14]), 'response_recode'] = 6 - survey_data_checked['responseNo']\n",
    "\n",
    "# Compute summed scores for satisfaction and trust scales\n",
    "satisfaction_scores = survey_data_checked[survey_data_checked['itemNo'].isin(satisfaction_items)].groupby('userID')['response_recode'].sum()\n",
    "trust_scores = survey_data_checked[survey_data_checked['itemNo'].isin(trust_items)].groupby('userID')['response_recode'].sum()\n",
    "\n",
    "# Add summed scores to the original DataFrame\n",
    "survey_data_checked = survey_data_checked.merge(satisfaction_scores.rename('satisfaction_score'), on='userID')\n",
    "survey_data_checked = survey_data_checked.merge(trust_scores.rename('trust_score'), on='userID')\n",
    "\n",
    "# Plot mean satisfaction scores per group\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=survey_data_checked, x='cond', y='satisfaction_score', palette=color_palette, errorbar='se')\n",
    "#sns.boxplot(data=survey_data_checked, x='cond', y='satisfaction_score',palette=color_palette)\n",
    "plt.title('Mean Satisfaction Scores by Condition (max=40)')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Satisfaction Score')\n",
    "plt.show()\n",
    "\n",
    "# Perform statistics following the now familiar logic: test for ANOVA assumptions, continue accordingly\n",
    "# generate factor columns\n",
    "survey_data_checked['referent'] = survey_data_checked['cond'].str[0]  # 's' or 'o'\n",
    "survey_data_checked['exp'] = survey_data_checked['cond'].str.split('-').str[1]  # 'cfe' or 'con'\n",
    "# Convert \"referent\" and \"exp\" columns to categorical type in Python\n",
    "survey_data_checked['referent'] = survey_data_checked['referent'].astype('category')\n",
    "survey_data_checked['exp'] = survey_data_checked['exp'].astype('category')\n",
    "\n",
    "### statistical analysis of satisfaction:\n",
    "# Check assumption 1) Normality (if Shapiro-Wilk p<0.05, assume data is NOT normal)\n",
    "normality_results = survey_data_checked.groupby('cond')['satisfaction_score'].apply(shapiro)\n",
    "normality_passed = all(p.pvalue > 0.05 for p in normality_results)\n",
    "print(\"Shapiro-Wilk Test for Normality:\")\n",
    "for cond, result in normality_results.items():\n",
    "    print(f\"Condition {cond}: W-statistic = {round(result.statistic,3)}, p-value = {round(result.pvalue,3)}\")\n",
    "\n",
    "# Check Homogeneity of Variances using Levene's test (if p<0.05, assume data is NOT homoscedastic)\n",
    "levene_stat, levene_pvalue = levene(*[survey_data_checked[survey_data_checked['cond'] == cond]['satisfaction_score'] for cond in survey_data_checked['cond'].unique()])\n",
    "homogeneity_passed = levene_pvalue > 0.05\n",
    "print(f\"\\nLevene's Test for Homogeneity of Variances: Statistic = {round(levene_stat,3)}, p-value = {round(levene_pvalue,3)}\")\n",
    "\n",
    "print(\"\\nVisualize results\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.pointplot(data=survey_data_checked, x='referent', y='satisfaction_score', hue='exp', palette=color_palette_interaction, dodge=True, markers=['o', 's'], capsize=0.1, errwidth=1, errorbar='se')\n",
    "plt.title('Interaction Plot: Mean Satisfaction Score by Referent and Exp')\n",
    "plt.xlabel('Referent')\n",
    "plt.ylabel('Mean Satisfaction Score')\n",
    "plt.legend(title='Exp')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if normality_passed and homogeneity_passed:\n",
    "    # Perform 2x2 ANOVA\n",
    "    model = ols('satisfaction_score ~ C(referent) * C(exp)', data=survey_data_checked).fit()\n",
    "    anova_results = anova_lm(model)\n",
    "    print(\"ANOVA Results:\")\n",
    "    print(round(anova_results,3))\n",
    "    print(\"\\nANOVA results table explained:\")\n",
    "    print(\"1. row: results for main effect of factor 'referent'\")\n",
    "    print(\"2. row: results for main effect of factor 'exp'\")\n",
    "    print(\"3. row: results for the interaction of both factors. If this is singificant, the effect of one factor depends on the value of the other factor.\")\n",
    "    print(\"4. row: residual, the variation in the dependent variable (accuracy) that isn't explained by the independent variables (referent or factor)\")\n",
    "    \n",
    "    # Perform post hoc test if interaction of ANOVA is significant\n",
    "    if anova_results[\"PR(>F)\"][2] < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_checked, val_col='satisfaction_score', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"\\nPost Hoc Comparisons (Interaction) using Dunn's Test:\")\n",
    "        print(round(posthoc_result,3))\n",
    "        \n",
    "else:\n",
    "    # Perform Kruskal-Wallis H Test for Accuracy\n",
    "    groups = [survey_data_checked[survey_data_checked['cond'] == cond]['satisfaction_score'] for cond in survey_data_checked['cond'].unique()]\n",
    "    kruskal_test_result = kruskal(*groups)\n",
    "\n",
    "    # Calculate Eta-Squared (η²)\n",
    "    H = kruskal_test_result.statistic\n",
    "    k = len(groups)\n",
    "    n = len(task_performance_cleaned)\n",
    "    eta_squared = (H - k + 1) / (n - 1)\n",
    "\n",
    "    print(\"\\nKruskal-Wallis H Test for Satisfaction Score:\")\n",
    "    print(f\"Statistic: {round(kruskal_test_result.statistic,3)}, p-value: {round(kruskal_test_result.pvalue,3)}\")\n",
    "    print(f\"Eta-Squared (η²) Effect Size: {round(eta_squared,3)}\")\n",
    "\n",
    "    # Perform Dunn's test for post hoc comparisons if Kruskal-Wallis test is significant\n",
    "    if kruskal_test_result.pvalue < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_checked, val_col='satisfaction_score', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"Post Hoc Comparisons using Dunn's Test with Bonferroni Correction:\")\n",
    "        print(round(posthoc_result,3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average judgments per group for Explanation Trust Scale\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, item in enumerate(trust_items, 1):\n",
    "    plt.subplot(2, 4, i)\n",
    "    sns.barplot(data=survey_data_checked[survey_data_checked['itemNo'] == item], x='cond', y='responseNo', palette=color_palette, errorbar='se')\n",
    "    plt.title(trust_labels[item])\n",
    "    plt.xlabel('Condition')\n",
    "    plt.ylabel('Mean Judgment')\n",
    "plt.suptitle('Mean Judgments for Explanation Trust Scale')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot mean trust scores per group\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=survey_data_checked, x='cond', y='trust_score', palette=color_palette, errorbar='se')\n",
    "plt.title('Mean Trust Scores by Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Trust Score')\n",
    "plt.show()\n",
    "\n",
    "### statistical analysis of trust:\n",
    "# Check assumption 1) Normality (if Shapiro-Wilk p<0.05, assume data is NOT normal)\n",
    "normality_results = survey_data_checked.groupby('cond')['trust_score'].apply(shapiro)\n",
    "normality_passed = all(p.pvalue > 0.05 for p in normality_results)\n",
    "print(\"Shapiro-Wilk Test for Normality:\")\n",
    "for cond, result in normality_results.items():\n",
    "    print(f\"Condition {cond}: W-statistic = {round(result.statistic,3)}, p-value = {round(result.pvalue,3)}\")\n",
    "\n",
    "# Check Homogeneity of Variances using Levene's test (if p<0.05, assume data is NOT homoscedastic)\n",
    "levene_stat, levene_pvalue = levene(*[survey_data_checked[survey_data_checked['cond'] == cond]['trust_score'] for cond in survey_data_checked['cond'].unique()])\n",
    "homogeneity_passed = levene_pvalue > 0.05\n",
    "print(f\"\\nLevene's Test for Homogeneity of Variances: Statistic = {round(levene_stat,3)}, p-value = {round(levene_pvalue,3)}\")\n",
    "\n",
    "print(\"\\nVisualize results\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.pointplot(data=survey_data_checked, x='referent', y='trust_score', hue='exp', palette=color_palette_interaction, dodge=True, markers=['o', 's'], capsize=0.1, errwidth=1, errorbar='se')\n",
    "plt.title('Interaction Plot: Mean Trust Score by Referent and Exp')\n",
    "plt.xlabel('Referent')\n",
    "plt.ylabel('Mean Trust Score')\n",
    "plt.legend(title='Exp')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if normality_passed and homogeneity_passed:\n",
    "    # Perform 2x2 ANOVA\n",
    "    model = ols('trust_score ~ C(referent) * C(exp)', data=survey_data_checked).fit()\n",
    "    anova_results = anova_lm(model)\n",
    "    print(\"ANOVA Results:\")\n",
    "    print(round(anova_results,3))\n",
    "    print(\"\\nANOVA results table explained:\")\n",
    "    print(\"1. row: results for main effect of factor 'referent'\")\n",
    "    print(\"2. row: results for main effect of factor 'exp'\")\n",
    "    print(\"3. row: results for the interaction of both factors. If this is singificant, the effect of one factor depends on the value of the other factor.\")\n",
    "    print(\"4. row: residual, the variation in the dependent variable (accuracy) that isn't explained by the independent variables (referent or factor)\")\n",
    "    \n",
    "    # Perform post hoc test if interaction of ANOVA is significant\n",
    "    if anova_results[\"PR(>F)\"][2] < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_checked, val_col='trust_score', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"\\nPost Hoc Comparisons (Interaction) using Dunn's Test:\")\n",
    "        print(round(posthoc_result,3))\n",
    "        \n",
    "elif not normality_passed and homogeneity_passed:\n",
    "    # Suppress R-specific output to avoid confusions:\n",
    "    with contextlib.redirect_stdout(open(os.devnull, 'w')), contextlib.redirect_stderr(open(os.devnull, 'w')):\n",
    "        # Enable the conversion between pandas DataFrames and R data frames\n",
    "        pandas2ri.activate()\n",
    "        # Convert columns to factors and prepare data for R\n",
    "        survey_data_checked_r = pandas2ri.py2rpy(survey_data_checked)\n",
    "        survey_data_checked_r = robjects.r['data.frame'](survey_data_checked_r)\n",
    "        robjects.r.assign(\"survey_data_importance_r\", survey_data_checked_r)\n",
    "        \n",
    "        robjects.r('''\n",
    "            survey_data_checked_r$referent <- as.factor(survey_data_checked_r$referent)\n",
    "            survey_data_checked_r$exp <- as.factor(survey_data_checked_r$exp)\n",
    "        ''')\n",
    "        \n",
    "        # Perform ART ANOVA using ARTool in R\n",
    "        art = importr('ARTool')\n",
    "        survey_data_checked_r = pandas2ri.py2rpy(survey_data_checked)\n",
    "        \n",
    "        formula = robjects.Formula('trust_score ~ referent * exp')\n",
    "        environment = formula.environment\n",
    "        environment['data'] = survey_data_checked_r\n",
    "        \n",
    "        # Perform the ART ANOVA\n",
    "        art_model = art.art(formula, data=survey_data_checked_r)\n",
    "        anova_results = art.anova_art(art_model,verbose=True)\n",
    "        \n",
    "    print(\"Aligned Rank Transform (ART) ANOVA Results:\")\n",
    "    print(anova_results)\n",
    "    print(\"\\nANOVA results table explained:\")\n",
    "    print(\"1. row: results for main effect of factor 'referent'\")\n",
    "    print(\"2. row: results for main effect of factor 'exp'\")\n",
    "    print(\"3. row: results for the interaction of both factors. If this is singificant, the effect of one factor depends on the value of the other factor.\")\n",
    "    print(\"\\nVisualize results\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.pointplot(data=survey_data_checked, x='referent', y='trust_score', hue='exp', palette=color_palette_interaction, dodge=True, markers=['o', 's'], capsize=0.1, errwidth=1, errorbar='se')\n",
    "    plt.title('Interaction Plot: Mean Trust Score by Referent and Exp')\n",
    "    plt.xlabel('Referent')\n",
    "    plt.ylabel('Mean Trust Score')\n",
    "    plt.legend(title='Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Perform post hoc test if interaction of ANOVA is significant\n",
    "    if anova_results.rx2('Pr(>F)')[2] < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_checked, val_col='trust_score', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"\\nPost Hoc Comparisons (Interaction) using Dunn's Test:\")\n",
    "        print(round(posthoc_result,3))\n",
    "        \n",
    "else:\n",
    "    # Perform Kruskal-Wallis H Test for Accuracy\n",
    "    groups = [survey_data_checked[survey_data_checked['cond'] == cond]['trust_score'] for cond in survey_data_checked['cond'].unique()]\n",
    "    kruskal_test_result = kruskal(*groups)\n",
    "\n",
    "    # Calculate Eta-Squared (η²)\n",
    "    H = kruskal_test_result.statistic\n",
    "    k = len(groups)\n",
    "    n = len(task_performance_cleaned)\n",
    "    eta_squared = (H - k + 1) / (n - 1)\n",
    "\n",
    "    print(\"\\nKruskal-Wallis H Test for the Trust Score:\")\n",
    "    print(f\"Statistic: {round(kruskal_test_result.statistic,3)}, p-value: {round(kruskal_test_result.pvalue,3)}\")\n",
    "    print(f\"Eta-Squared (η²) Effect Size: {round(eta_squared,3)}\")\n",
    "\n",
    "    # Perform Dunn's test for post hoc comparisons if Kruskal-Wallis test is significant\n",
    "    if kruskal_test_result.pvalue < 0.05:\n",
    "        posthoc_result = sp.posthoc_dunn(survey_data_checked, val_col='trust_score', group_col='cond', p_adjust='bonferroni')\n",
    "        print(\"Post Hoc Comparisons using Dunn's Test with Bonferroni Correction:\")\n",
    "        print(round(posthoc_result,3))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
